%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Uncertainty Quantification}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\author{
    \textbf{Alexandros Taflanidis}
    \and Joel P. Conte
    \and George Deodatis
    \and Sanjay Govindjee
    \and Michael Shields}
\tocauthor{}
\authorrunning{Taflanidis et al.}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
%\institute{Name of First Author \at Name, Address of Institute, %\email{name@email.address}
%\and Name of Second Author \at Name, Address of Institute %\email{name@email.address}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle
\label{chapter:uq}

Uncertainty quantification (UQ) is a rapidly evolving scientific field, with advances in computer science and statistical computing promoting constant developments in the way uncertainty is incorporated in the predictive analysis of engineering systems \citep{smith2013uncertainty}. In particular, over the last decade(s) the popularity of HPC and of machine-learning tools have dramatically impacted the way computational simulation is utilized within the UQ field, lifting many barriers that were traditionally associated with simulation-based UQ techniques, and allowing the detailed characterization and propagation of uncertainties even for problem with highly complex (computationally intensive) numerical models. It is the current consensus within the UQ community that these advances will (or have already) remove(d) the need for simplified approaches with respect to both the uncertainty characterization (assumptions/models used to describe uncertainty and system performance) or uncertainty propagation (estimation of statistics of interest). 

When discussing computational advances and state-of-the art tools in UQ, greater emphasis is typically placed on algorithmic approaches rather than the corresponding software facilitating the implementation of these approaches. The reason for this is that development of scientific tools for UQ has focused traditionally on a specific UQ sub-field (for example, surrogate modeling to support UQ analysis \citep{lophaven2002dacea,gorissen2010surrogate}), with a large number of researchers \citep[e.g.,][]{bect2017bayesian,clement2018methods} offering open-source algorithms to even address a specific class of problems within each of these sub-fields. Many of these algorithms have been developed in MATLAB, though in recent years significant emphasis has been placed on open-source libraries developed using Python (or less frequently C++ or R) and typically distributed though GitHub.    

Since UQ is a very broad field, here discussions focus on applications within the natural hazards engineering field, with some references to relevant general UQ advances also offered. Emphasis is on computational aspects, the most pertinent UQ feature for a state-of-the-art review of UQ simulation methods. Additionally, discussions focus on algorithmic developments, with some references also on relevant software. With respect to description of uncertainty emphasis is placed on probabilistic characterization; even though alternative approaches exist \citep{beer2013imprecise}, such as use of fuzzy sets and interval analysis, the current standard of practice in natural hazards engineering is to rely on probabilistic UQ analysis. This can be attributed to the tradition in civil engineering codes to describe performance with respect to statistical measures (probability of exceeding performance limit states), or to the fact that hazard exposure, the most significant source of variability when discussing risk in the natural hazards engineering (NHE) context, is almost always described using probabilistic measures \citep{mcguire2004seismic, resio2007white}. 

Should be stressed that all references provided here are merely indicative ones, although a few of them can be regarded as seminal work, since the UQ field, even with respect to natural hazards engineering applications, is very broad and constantly expanding.    

\section{Uncertainty Characterization}
\label{sec:uq_characterization}

In natural hazards engineering, characterization of the uncertainties impacting predictions is integrally related to risk quantification. Performance based engineering (PBE) \citep{whittaker2003performancebased, goulet2007evaluation, riggs2008experimental, ciampoli2011performancebased, barbato2013performancebased, fischer2019performancebased} represents undoubtedly the foundational development for this task. Performance-based engineering decouples the risk quantification to its different components, mainly hazard analysis (exposure), structural analysis (vulnerability), and damage and loss analysis (consequences), with uncertainties included (and impacting predictions) in all these components. Variability of the hazard itself, in terms of both occurrence and intensity, is widely acknowledged to correspond to the most significant source of uncertainty in this setting. Frequently hazard variability is represented through a resultant IM \citep{baker2005vectorvalued,kohrangi2016implications}, though comprehensive approaches that focus on connecting the excitation to parameters of the geophysical process creating it also exist, for example in earthquake engineering description of time-histories through use of stochastic ground motion models dependent on seismological parameters \citep{bijelic2018validation, vlachos2018predictive} or in coastal risk estimation use of surge modeling numerical tools dependent on atmospheric storm characteristics \citep{resio2007white}. Beyond the hazard variability, uncertainties related to parameters of the structural model or generalized system model (for applications not examining directly structural risk) and to the characteristics for describing performance are also recognized as important for inclusion in risk estimation \citep{porter2002sensitivity}. The term ``system'' will be used herein to describe the application of interest; this may pertain, for example, to a building model, to an infrastructure network, or to a soil-structure interaction system configuration. 

Uncertainty within this natural hazards engineering risk characterization setting is ultimately described through a discrete number of parameters (treated as random variables) pertaining to either the hazard or the system/performance model,including parameters to describe interdependencies and deterioration characteristics \citep{jia2018statedependent,akiyama2020lifecycle}. Even when the uncertainty description for the underlying problem actually entails a stochastic sequence/process or a random field, a discretized approximation of these functions is commonly utilized, as necessitated by the numerical tools used to compute the system response \citep{gidaris2014surrogate}. This translates into use of a parameterized realization for the excitation or model characteristics, an approach that seamlessly fits within the overall PBE framework. Exceptions exist primarily for stochastic dynamics problems, for which propagation of the stochastic excitation uncertainty can be performed using random vibration theory, such as exact or approximate solution of stochastic differential equations or estimation of stationary statistics in the frequency domain \citep{li2009stochastic}. Though such approaches offer substantial benefits, their implementation is primarily constrained to linear systems or nonlinear systems with moderate degree of complexity \citep{dossantos2016incremental, wang2016tailequivalent}, such as systems with very small number of degrees-of-freedom or nonlinearities having simple, analytical form. As such, their utility within natural hazards engineering is limited to specialized applications. Even in such cases, the remaining uncertainties, beyond the stochastic excitation itself, must be described using a parametric description. The overall parameterized uncertainty description promoted within PBE is therefore well aligned with such approaches, as their adoption simply requires substitution of the deterministic simulation system model with a stochastic simulation system model, the latter representing the solution of the stochastic dynamics problem.

When using Monte Carlo simulation (MCS) techniques to propagate uncertainty (see discussion in the next paragraph), a critical part of the methodology is the numerical generation of sample functions of the stochastic processes, fields, and waves involved in the problem, modeling the uncertainties in the excitations (e.g., wind velocities, seismic ground motion, and ocean waves) and in the structural system (e.g., material, mechanical and geometric properties). These processes, fields, and waves can be stationary or non-stationary, homogeneous or non-homogeneous, scalar or vector, 1D or multi-dimensional, Gaussian or non-Gaussian, or any combination of the above. It is crucial for a simulation algorithm to be computationally efficient as a very large number of sample functions might be needed. A wide range of methodologies is currently available to parametrically describe uncertainty and perform these simulations including the spectral representation method \citep{li1991simulation, shinozuka1991simulation, shields2011simple, benowitz2015simulation}, Karhunen-Loeve expansion and polynomial chaos decomposition \citep{ghanem1991stochastic}, auto-regressive moving-average models \citep{spanos1983arma,deodatis1988autoregressive}, local average subdivision method \citep{fenton1990simulation}, wavelets \citep{zeldin1996random}, Hilbert transform techniques \citep{wang2014modeling}, and turning bands methods \citep{mantoglou1982turning}.

The setting outlined in the previous two paragraphs leads, ultimately, to risk characterized as a multidimensional integral over the parametric uncertainty description (input), with uncertainty propagation (output) translating to estimation of the relevant statistics (estimation of integrals representing moments or failure probabilities with respect to different limit states). Though proper definition of the statistics of interest, different attitudes towards risk can be furthermore described, including risk aversion which is especially relevant for natural hazards engineering applications \citep{cha2012riskaverse}. The aforementioned integral is frequently expressed with respect to the conditional distributions of the different resultant risk components \citep{goulet2007evaluation,barbato2013performancebased}, for example {hazard / response given hazard / consequences given response}. This represents merely a simplification for risk quantification purposes as allows for the decoupling of the different components. Even when this simplification is invoked, risk fundamentally originates from the uncertainty in the model parameters of the problem formulation (including hazard/system/performance description), quantified by assigning a probability distribution to them, representing the UQ input. It should be noted that in many natural hazards engineering applications some aspects of the performance (and of the associated risk) are described by utilizing resultant statistical models instead of explicitly addressing the underlying sources of uncertainty; for example, in PBE fragility functions are frequently leveraged to describe the combined effect of the uncertainties influencing the parameters of capacity and demand models, while in loss estimation resultant distributions are leveraged to encompass the multiple sources of uncertainty influencing consequence quantification. In such cases the integral quantifying risk is expressed with respect to the remaining parametric uncertainty sources.      . 

\section{Uncertainty Propagation}
\label{sec:uq_propagation}

For uncertainty propagation, the traditional approach in natural hazards engineering has been the use of point estimation methods, either methods that focus on the most probable values of the model parameters like the first-order second moment (FOSM) method \citep{baker2008uncertainty} and its variants \citep{vamvatsikos2013derivation}, or methods that focus on the peaks of the integrand of the probabilistic integral (design points) like the first- and second-order reliability methods (FORM/SORM) \citep{koduru2010feasibility}. As point estimation methods are inherently approximate, with no available means to control their accuracy, advances in computer science and statistics, including the development of innovative MCS techniques, such as Subset Simulation \citep{au2003subset}, Efficient Global Reliability Analysis (EGRA) \citep{bichon2013efficient}, and Adaptive Kriging with Monte Carlo Simulation (AK-MCS) \citep{echard2011akmcs}, have encouraged researchers the past decade to rely more heavily on Monte Carlo simulation (MCS) tools for uncertainty propagation in NHE \citep{smith2011monte, taflanidis2011simulationbased, vamvatsikos2014seismic, esposito2015simulation, deb2019parametric}. 

Although point estimation methods do still maintain utility and popularity, NHE trends follow the broader UQ community trends in promoting computer and MCS approaches, as these techniques facilitate high accuracy uncertainty propagation (unbiased estimation) with limited fundamental constraints on the complexity of the probability and numerical models used. Of course, computational complexity is still a concern for MCS, especially for applications with high-dimensional uncertainties and challenging quantities of interest (such as rare event simulation), creating in many instances practical constraints for the efficient implementation. The current state of the art in NHE for addressing these constraints is to leverage both advanced MCS techniques \citep{au2003subset,li2017system,bansal2018subset} but, more importantly, machine learning and advanced computational statistics tools \citep{echard2011akmcs, abbiati2017hierarchical, ding2018multifidelity, su2018efficient, wang2018bayesian}. Relevant recent advances for MCS focus on variance reduction techniques, e.g., Latin hypercube sampling \citep{vamvatsikos2014seismic}, stratified sampling \citep{jayaram2010efficient}, importance sampling \citep{papaioannou2018reliability}, Markov chain Monte Carlo methods \citep{au2003subset} and sequential approaches for rare event simulation \citep{jia2017new}, with substantial emphasis on problems with high-dimensional uncertainties \citep{au2003subset,wang2016crossentropybased}, whereas for machine learning focus is primarily on use of a variety of surrogate modeling (metamodeling) techniques \citep{stern2017accelerated, zhang2018adaptive, bernier2019fragility, gentile2020gaussian, le2020neural, zhang2020physicsguided}. Many machine learning implementations in natural hazards engineering fall under the category of direct adoption of techniques developed by the broader UQ community, though number of studies do address challenges unique to the integration of surrogate modeling in natural hazards engineering problems, for example the need to address high-dimensionality of input when stochastic description is utilized for non-stationary excitations \citep{gidaris2015kriging}.  

Note: the natural hazards engineering modeling community has been continuously increasing the complexity of the models they adopt. Such high fidelity numerical models, able to capture the behavior of structural, geotechnical and soil-foundation-structural systems all the way to collapse or the brink of collapse, are inherently nonlinear hysteretic (path-dependent) and frequently degrading/softening. Therefore, such models present (significant) challenges in term of robustness of convergence of the iterative schemes used to integrate their equations of motion. The significance of these challenges will further increase in the MCS based UQ context and requires significant research efforts to be overcome. One of the implicit outcomes of this complexity increase is a further increase of the input/output dimensionality in natural hazards engineering UQ problems. This leads to applications with high-dimensional uncertainties that, traditionally, pose challenges for UQ algorithms \citep{au2003importance,schueller2004critical}. Different approaches have been explored to address this challenge within natural hazards engineering applications, ranging from specialized MCS algorithms \citep{au2001estimation, wang2016crossentropybased}, to the formal integration of dimensional reduction techniques \citep{jia2013kriging}, to the use of global sensitivity analysis indices to the selection of subsets of inputs to emphasize in different MCS schemes \citep{jia2014adaptive}. It is evident, though, that stronger emphasis will be required on this topic in the future.     

Discussing more broadly advances in the UQ field, emphasis is currently strongly placed on machine learning techniques for accelerating UQ computations \citep{murphy2012machine, ghanem2017handbook, tripathy2018deep}. The relevant developments are frequently integrated with advanced MCS techniques, for example for topics like rare event simulation \citep{li2011efficient,balesdent2013krigingbased,bourinet2016rareevent}. With respect directly to machine learning, some emphasis is given on the approaches for tuning and validation \citep{mehmani2018concurrent}, though the primary focus is on the proper design of the computer simulation experiments (DoE) \citep{kleijnen2008design, picheny2010adaptive, kyprioti2020adaptive} that are used to inform the development of the relevant computational statistics tools. Adaptive DoE is widely acknowledged to offer substantial advantages in balancing computational efficiency and accuracy for UQ analysis when machine learning techniques are used, and significant research efforts are currently focused on advancing DoE techniques, remaining an open challenge for the community. It should be noted that characteristics of the adaptive DoE depend on the utility of the surrogate model \citep{liu2018survey}, whether is it intended to serve as a global replacement of the original numerical model (i.e., develop the surrogate model first and then leverage it to perform different UQ tasks) or be used for a very specific UQ tasks (for example estimate reliability index for a specific limit state). 

The concept of model fidelity remains unexplored within the natural hazards engineering community, but it plays a central role in modern UQ techniques, with a range of algorithms developed to properly integrate hierarchical fidelity models to promote efficient and accurate uncertainty propagation \citep{geraci2017multifidelity, peherstorfer2018survey}. Combination of machine-learning (primarily surrogate modeling) techniques with different fidelity models is also a topic that has been receiving increasing attention for facilitating the use of expensive numerical models in UQ \citep{debaar2015uncertainty, zhou2016active}. In the natural hazards engineering setting, discussions on explicitly exploiting model fidelity for risk estimation are very limited; therefore, the community still heavily emphasizes use of high-fidelity models without, yet, examining how different levels of simulation fidelity and the use of reduced order models can be properly combined to promote efficient and accurate risk estimation. Multi-fidelity Monte Carlo and hierarchical surrogate modeling techniques constitute, undoubtedly, important opportunity areas for advancing UQ analysis in natural hazards engineering.

Another important aspect of uncertainty propagation is the concept of sensitivity analysis. In natural hazards engineering this has been primarily implemented as local sensitivity analysis, i.e., estimation of gradient information \citep{haukaas2007methods,gu2009finite},  since this fits well with the point estimation methods used frequently for calculation of statistics (helps the identification of design points). The more relevant, though, within a UQ setting is global sensitivity analysis \citep{sobol1990sensitivity,saltelli2002making,rahman2016fsensitivity}, as it allows identification of the relative importance of the different sources of uncertainty, offering insights with respect to both accelerating UQ computations (helping, in particular, with high-dimensional applications as discussed earlier), as well as to understanding of the critical factors impacting the overall risk. Though global sensitivity analysis can be particularly useful for hazard applications \citep{vetter2012global}, it is currently receiving limited practical interest within natural hazards engineering, though implementations do exist even for all purpose codes, for example \citep{bourinet2009review}. More formal integration of global sensitivity analysis tools within the natural hazards engineering community represents, therefore, another topical area that advancements can/should be made. Of course the computational cost for global sensitivity analysis, for example calculation of first and higher order sensitivity indexes, is much higher than the cost of simple uncertainty propagation; relevant techniques range from use of quasi-Monte Carlo \citep{saltelli2002making} to surrogate modeling \citep{sudret2008global} to sample-based methods relying on approximation of conditional distributions \citep{li2016efficient,hu2019probability}.  

\section{Model Calibration and Bayesian Inference}
\label{sec:uq_calibration}

Model updating/calibration plays an important role in natural hazards engineering, with data coming from both component (or system) -level experiments or system-level observations during (or post) actual excitation conditions. Within UQ setting, the current standard to perform this updating is Bayesian inference \citep{beck2010bayesian,kontoroupi2017online}. Using observation data, Bayesian inference can be leveraged to provide different type of outputs/results \citep{beck2013prior} through the following three tasks: (1) identify the most probable model parameters or even update the entire probability density function for these parameters (obtain posterior distributions); (2) perform posterior predictive analysis and update risk using the new information; and (3) when different numerical models are examined, identify the probability of each of them (as inferred by the data) to either select the most appropriate or calculate the weights when all of them will be used in a model averaging setting (model class selection). The typical implementation refers to model parameter updating, what is traditionally viewed as model calibration, with model class selection less frequently used, especially within natural hazards engineering community applications. Still, Bayesian model class selection offers a comprehensive tool for evaluating appropriateness of different models \citep{muto2008bayesian}, and especially for natural hazards engineering applications can be integrated with health monitoring tools \citep{oh2018bayesian}. Ideally, model parameter updating should be performed accounting for pertinent sources of real-world uncertainties (e.g.: noisy input-output measurements, uncertainty in model parameters, model form uncertainty, environmental variability). Formulating the likelihood equation to account for such uncertainties is a crucial part of the Bayesian model updating/calibration process. The so-called Kennedy O'Hagan framework has emerged as a robust approach to account for all these uncertainties, especially the model form error \citep{kennedy2001bayesian}. 

From computational perspective, Bayesian updating can have significant computational burden, especially when complex finite element (FE) models are utilized. Such FE models contain numerous unknown parameters which drastically increases the computational cost of the Bayesian updating process. This necessitates the use of identifiability and sensitivity analyses prior to model updating to select the most significant/influent parameters to use in the updating process. This step tremendously improves the calibration process runtime of complex FE models with numerous parameters and results in better parameter estimation results \citep{ramancha2021bayesianupdating}. In addition, non-identifiable parameters result in non-unique parameter estimates \citep{ramancha2020nonunique}. Fisher information based local identifiability analysis and variance-based global sensitivity analysis are commonly used methods for parameter screening to identify the most significant/influent and identifiable parameters \citep{ramancha2021bayesianupdating}. Beyond this critical dimensionality reduction, a variety of algorithmic approaches are commonly used to address the computational complexity in Bayesian updating applications. Common approaches include the use of advanced MCS techniques to reduce the total number of simulations needed \citep{quiroz2018speeding}, the integration of metamodeling to approximate the complex system model \citep{angelikopoulos2015xtmcmc, giovanis2017bayesian, wang2019reliabilitybased, zhang2019accelerating} or the use of direct differentiation tools to accelerate computations \citep{astroza2017batch}. The second candidate implementation, the use of surrogate modeling techniques, has undoubtedly significant potential in accommodating the use of complex models within Bayesian calibration schemes.     

Bayesian updating may rely on point estimates, equivalent to identifying and using only the most probable (based on the observation data) model parameter values (expressed as a nonlinear optimization problem), or leverage the entire posterior distribution (expressed as problem of sampling from this distribution). For the latter, Markov Chain Monte Carlo (MCMC) techniques need to be used for any of the three tasks entailed in Bayesian inference \citep{catanach2018bayesian}. In particular, transitional MCMC (TMCMC) is a versatile method for sampling the posterior distribution \citep{ching2007transitional,betz2016transitional} and due to its computationally parallel nature, is ideal for Bayesian inference of computationally expensive FE models \citep{ramancha2021bayesian, ramancha2021bayesianupdating}. For problems involving inference for dynamical models (which majority of applications in natural hazards engineering fall under), updating can be done in batch mode, using all observation data at once, or recursive mode, sequentially updating model characteristics during the time-history for the observations \citep{astroza2017batch,ramancha2021bayesianupdating}. The batch approach is a direct implementation of the broader Bayesian inference framework. The recursive implementation typically leads to filtering approaches, including Kalman filters (KF) and its variants (Extended KF or Unscented KF) that relies on linear or Gaussian assumptions \citep{astroza2017batch,kontoroupi2017online,erazo2018bayesian}, and particle filters (PF) that reproduces the work of KF in non-linear and/or non-Gaussian environment by a sequential MCS approach \citep{chatzi2009unscented, wei2013dynamic, olivier2017particle}. Recursive approach is used primarily for real-time or online applications, focusing mainly on the most probable parameter values. 

\section{Design under Uncertainty}
\label{sec:uq_design}

In natural hazards engineering, design under uncertainty has been traditionally expressed as a reliability-based design optimization (RBDO) \citep{spence2012large, chun2019systemreliabilitybased} or as a robust design optimization (RDO) \citep{greco2015robust} problem. Some recent approaches deviate from this pattern and follow directly PBE advances, formulating the design problem with respect to life-cycle cost and performance objectives \citep{shin2014minimum}, even adopting multiple probabilistic criteria to represent different risk-attitudes \citep{haukaas2012reliabilitybased, gidaris2017multiobjective, li2018probabilistic, deb2019simplified}. Practical applications focus on design of supplemental dissipative devices \citep{shin2014minimum, gidaris2017multiobjective, altieri2018reliabilitybased} and member-sizing \citep{huang2015performancebased, suksuwan2018optimization}, or even on topology-based optimization of structural systems \citep{bobby2017reliabilitybased, zhu2017topology}.

With respect to the solution of the corresponding optimization problem, the natural hazards engineering community follows the broader UQ trends. Design under uncertainty optimization problems undoubtedly present significant computational challenges since they combine two tasks, each with considerable computational burden: uncertainty propagation and optimization. Discussed next is how uncertainty propagation is handled within this coupled problem.

Common approaches, especially within context of RBDO and RDO, typically rely on approximate point-estimation methods like FORM/SORM \citep{papadimitriou2018reliability} and some sort of decoupling of the optimization/uncertainty-propagation loops to accelerate convergence \citep{beyer2007robust}. Advances in the use of simulation techniques within UQ have created new opportunities the past decade to incorporate MCS techniques in solving design under uncertainty problems \citep{spall2003introduction,flint2016developing}, lifting some of the traditionally associated computational barriers. Greater emphasis is continuously placed on solving design under uncertainty problems using advanced Monte Carlo techniques \citep{medina2014adaptive}, frequently coupled with an intelligent integration of surrogate modeling tools \citep{dubourg2011reliabilitybased,bichon2013efficient,zhang2018adaptive}. It is expected that this trend will continue, since computer science and machine learning advances have dramatically altered the computational complexity for leveraging MCS for design optimization under uncertainty, offering an attractive alternative to traditional approaches that relied on the approximate (but highly efficient) point estimation methods.   

\section{Relevant Software}
\label{sec:uq_tools}

Beyond specific UQ algorithms developed by individual researchers and shared in repositories like GitHub or MATLAB's File Exchange, two other important UQ software categories exist:

\begin{itemize}
    \item Libraries integrated with existing modeling tools appropriate for natural hazards engineering analysis, like the general purpose FE model reliability tools offered through FERUM \citep{bourinet2009review}. These libraries frequently address a specific type of UQ analysis, e.g., direct MCS or reliability estimation.
    \item Software that looks at UQ analysis with a broad brush could be appropriate for use in natural hazards engineering applications (but as of yet has not necessarily been developed specifically for that purpose). Such software typically covers the entire range of UQ analysis, with continuous integration of the relevant state-of-the art advances. They are composed of scientific modules that perform different UQ tasks, connected through the main software engine and, in addition, are commonly equipped with an appropriate GUI. 
\end{itemize}
	
The last category of UQ software packages is of greater interest, especially since it covers the entire domain of a rapidly expanding field and facilitates the integration of the relevant developments, which typically leverage different classes of tools (e.g., rare-event simulation using surrogate models with adaptive refinement. UQ software programs typically address the following tasks:

\begin{description}
    \item[Probabilistic modeling:]{This pertains to standard uncertainty characterization, extending from simple parametric description to stochastic characterization (including dimensionality reduction), and represents the input to the UQ software.}
    \item[:]{These tasks encompass direct MCS with Latin hypercube sampling, use of point estimation methods (FORM/SORM),  variance reduction, and rare event simulation. UQ outputs considered correspond typically to statistical moments, probabilities of exceedance for different limit states, or fitted distributions. The numerous software programs adopt different tools for the aforementioned tasks and most lack a complete adaptive implementation; some degree of competency on behalf of the end-user for selecting appropriate algorithms and parameters is assumed. Many types of software have started recently to integrate multi-fidelity MCS approaches.}
    \item[Surrogate modeling:]{Common classes of metamodels used include Gaussian processes, polynomial chaos, support vector machines, and radial basis functions. The developed surrogate models can be then leveraged within the software to accelerate computations for other UQ tasks. Adaptive DoE options are typically available and used frequently; standard DoE approaches are not, however, necessarily tailored to the specific UQ task the end-user is interested in applying. Most software programs sacrifice robustness (an approach that is reliable and works independent of the end-user competency) for efficiency (the ability to develop high-accuracy metamodels with the least number of simulation experiments).}
    \item[Global sensitivity analysis:]{This is typically performed through calculation of Sobol indices (UQ output) using some approximate (quasi-Monte Carlo) technique or surrogate modeling (polynomial chaos expansion).}
    \item[Data analysis and model calibration, with some emphasis on Bayesian inference techniques:]{Although Bayesian updating is very common, model class selection is not. Addressing modeling complexity remains a bigger challenge for Bayesian inference applications since integrating metamodeling techniques is not trivial. The challenge here is to establish a fully automated integration that can address different degrees of competency for the end-user and a wide range of application problems with certain degree of robustness (impact of metamodel error). For problems with dynamical models, the typical approach is to use the batch updating method since that leads to a common broader Bayesian inference framework.}
    \item[Design-under-uncertainty:]{Though this is not a common option, some software programs do offer the ability to perform some form of optimization under uncertainty and are most applicable to RBDO and RDO problems. Integrating state-of-the-art MCS techniques in this setting remains also a challenge. Implementations are typically computationally expensive or rely on approximate approaches for the uncertainty propagation.}
\end{description}

\noindent Out of the different UQ software programs that exist, the following are worth direct mention as representing the state-of-the-art: 
% \newline

\paragraph{DAKOTA} Developed by the Sandia National Laboratory and written in C++, \citeprgm{DAKOTA} is widely considered as the standard for UQ software and delivers both state-of-the-art research and robust, usable tools for optimization and UQ \citep{adams2009dakota}. It has a range of algorithms for all aforementioned UQ tasks and has a wide community that supports its continuous development. 

\paragraph{OpenTURNS} It is an open-source (C++/Python library) initiative for the treatment of uncertainties and risk in simulations \citep{andrianov2007open}. It addresses all aforementioned UQ tasks apart from design under uncertainty. 

\paragraph{UQLab} Developed at ETH Zürich, \citeprgm{UQLab} is a MATLAB-based general purpose UQ framework \citep{marelli2014uqlab}. Like OpenTURNS it addresses all the aforementioned UQ tasks apart from design under uncertainty. 

\paragraph{OpenCOSSAN} 
\citeprgm{OpenCOSSAN} is the MATLAB based open-source version of the commercial software \citeprgm{COSSAN-X} \citep{patelli2017cossan}, which was initially developed to integrate UQ and reliability techniques within FEM analysis, with modules that extend across all aforementioned UQ tasks.

\paragraph{UQpy} It is an continuously expanding open source (Python based) library of tools for UQ analysis \citep{olivier2020uqpy}. It addresses all UQ tasks apart from design-under-uncertainty (at least at its current form).

\paragraph{MUQ} 
\citeprgm{MUQ} is an open source (C++/Python library) collection of tools for UQ analysis, with emphasis on probabilistic modeling, Bayesian inference and surrogate modeling. Developments in MUQ have been slightly more accelerated recently, though overall it has not followed the same pace as other open-source initiatives mentioned above (\url{http://www.cossan.co.uk/software/open-cossan-engine.php}).

Beyond these specific software or open source tool collections, there is an increasing number of Python based open-source libraries that are offered by researchers for UQ analysis, for example UQ-Pyl (http://www.uq-pyl.com/), FilterPy (https://github.com/rlabbe/filterpy), Surrogate Modeling Toolbox (https://github.com/SMTorg/SMT). Most of them are focusing on specialized UQ tasks (or groups of tasks) and do not intend to establish a generalized UQ analysis workflow. The degree of ongoing improvement and bug fixes also varies substantially between these efforts. 

