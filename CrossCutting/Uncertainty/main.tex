%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Uncertainty Quantification}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\author{
    \textbf{Alexandros Taflanidis}
    \and \textbf{Joel P. Conte}
    \and \textbf{George Deodatis}
    \and \textbf{Sanjay Govindjee},
    \newline
    along with review comments and suggestions by Aakash B. Satish, Michael D. Shields, Lance Manuel, and Sang-ri Yi
    }
\tocauthor{}
\authorrunning{Taflanidis et al.}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
%\institute{Name of First Author \at Name, Address of Institute, %\email{name@email.address}
%\and Name of Second Author \at Name, Address of Institute %\email{name@email.address}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle
\label{chapter:uq}

Uncertainty quantification (UQ) is a rapidly evolving scientific field, with advances in computer science and statistical computing promoting constant developments in the way uncertainty is incorporated in predictive analysis of engineering systems \citep{smith2013uncertainty}. In particular, over the last few decades, the popularity of high-performance computing (HPC) and machine-learning tools have dramatically impacted the way computational simulation is utilized within the UQ field, lifting many barriers that were traditionally associated with simulation-based UQ techniques, and allowing the incorporation of detailed characterization and propagation of uncertainties in computationally intensive numerical models to solve highly complex problems. The current consensus within the UQ community is that these advances will remove or have already removed the need for simplified approaches with respect to both the uncertainty characterization (assumptions/models used to describe uncertainty and system performance) or uncertainty propagation (estimation of statistics of interest). 

When discussing computational advances and state-of-the art tools in UQ, greater emphasis is typically placed on algorithmic approaches than the corresponding software facilitating the implementation of these approaches. The reason for this is that development of scientific tools for UQ has focused traditionally on a specific UQ sub-field [for example, surrogate modeling to support UQ analysis \citep{lophaven2002dacea, gorissen2010surrogate}], with a large number of researchers \citep[e.g.,][]{bect2017bayesian, clement2018methods} offering open-source algorithms to address a specific class of problems even within each of these sub-fields. Although many of these algorithms have been developed in MATLAB, in recent years significant emphasis has been placed on open-source libraries developed using Python (or less frequently C++ or R) and typically distributed though GitHub.    

Since UQ is a very broad field, discussions herein focus on applications within the natural hazards engineering (NHE) domain, with some references to relevant general UQ advances also offered. Emphasis is on computational aspects, the most pertinent UQ feature for a state-of-the-art review of UQ simulation methods. Additionally, discussions focus on algorithmic developments, with some references also on relevant software. With respect to description of uncertainty, emphasis is placed on probabilistic characterization. Alternative approaches exist \citep{beer2013imprecise}, such as use of fuzzy sets and interval analysis, however, the current standard of practice in NHE is to rely on probabilistic UQ analysis. This can be attributed to the tradition in civil engineering design codes to describe performance with respect to statistical measures (probability of exceeding performance limit states), or because hazard severity, the most significant source of variability when discussing risk in the context of NHE, is almost always described using probabilistic measures \citep{mcguire2004seismic, resio2007white}. 

Note: all references provided here are examples and general in scope; several of them can be regarded as seminal work since the UQ field---even with respect to NHE applications---is very broad and constantly expanding.    

\section{Uncertainty Characterization}
\label{sec:uq_characterization}

In NHE, characterization of the uncertainties impacting predictions is integrally related to risk quantification. Performance-based engineering (PBE) \citep{whittaker2003performancebased, goulet2007evaluation, riggs2008experimental, ciampoli2011performancebased,  barbato2013performancebased, fischer2019performancebased} represents undoubtedly the foundational development for this task. Performance-based engineering decouples the risk quantification to its different components, mainly hazard and exposure analysis, structural analysis (vulnerability), and loss analysis (consequences), with uncertainties included (and impacting predictions) in all these components. Variability of the hazard itself, in terms of both occurrence and intensity, is widely acknowledged to correspond to the most significant source of uncertainty in this setting. Frequently, hazard variability is represented through a resultant intensity measure (IM) \citep{baker2005vectorvalued, kohrangi2016implications}, though comprehensive approaches that focus on connecting the excitation to parameters of the geophysical process creating it also exist. For example, in earthquake engineering, the description of time-histories through stochastic ground motion models dependent on seismological parameters \citep{bijelic2018validation, vlachos2018predictive}; in coastal risk estimation surge modeling numerical tools are dependent on atmospheric storm characteristics \citep{resio2007white}. Beyond the hazard variability, uncertainties related to parameters of the structural model or generalized system model (for applications not examining directly structural risk) and to the characteristics for describing performance are also recognized as important for inclusion in risk estimation \citep{porter2002sensitivity}. The term ``system'' will be used herein to describe the application of interest; e.g., this may pertain to a building model, to an infrastructure network, or to a soil--structure interaction system configuration. 

Uncertainty within this NHE risk characterization setting is ultimately described through a discrete number of parameters (treated as random variables) pertaining to either the hazard or the system/performance model---including parameters---to describe interdependencies and deterioration characteristics \citep{jia2018statedependent, akiyama2020lifecycle}. Even when the uncertainty description for the underlying problem actually entails a stochastic sequence/process or a random field, a discretized approximation of these functions is commonly utilized, as necessitated by the numerical tools used to compute the system response \citep{gidaris2014surrogate}. This translates into use of a parameterized realization for the excitation or model characteristics, an approach that seamlessly fits within the overall PBE framework. Exceptions exist primarily for stochastic dynamics problems, for which propagation of the stochastic excitation uncertainty can be performed using random vibration theory, such as exact or approximate solution of stochastic differential equations or estimation of stationary statistics in the frequency domain \citep{li2009stochastic}. Though such approaches offer substantial benefits, their implementation is primarily constrained to linear systems or nonlinear systems with moderate degree of complexity \citep{dossantos2016incremental, wang2016tailequivalent}, such as systems with very small number of degrees-of-freedom or nonlinearities having a simple, analytical form. As such, their utility within NHE is limited to specialized applications. Even in such cases, the remaining uncertainties, beyond the stochastic excitation itself, must be described using a parametric description. The overall parameterized uncertainty description promoted within PBE is therefore well aligned with such approaches, as their adoption simply requires substitution of the deterministic simulation system model with a stochastic simulation system model, the latter representing the solution of the stochastic dynamics problem.

When using Monte Carlo simulation (MCS) techniques to propagate uncertainty (see discussion in the next paragraph), a critical part of the methodology is the numerical generation of sample functions of the stochastic processes, fields, and waves involved in the problem, modeling the uncertainties in the excitations (e.g., wind velocities, seismic ground motion, and ocean waves) and in the structural system (e.g., material, mechanical, and geometric properties). These processes, fields, and waves can be stationary or non-stationary, homogeneous or non-homogeneous, scalar or vector, one dimensional or multi-dimensional, Gaussian or non-Gaussian, or any combination of the above. It is crucial for a simulation algorithm to be computationally efficient as a very large number of sample functions might be needed. A wide range of methodologies is currently available to parametrically describe uncertainty and perform these simulations including the spectral representation method \citep{li1991simulation, shinozuka1991simulation, shields2011simple, benowitz2015simulation}, Karhunen-Loeve expansion and polynomial chaos decomposition \citep{ghanem1991stochastic}, auto-regressive moving-average models \citep{spanos1983arma, deodatis1988autoregressive}, local average subdivision method \citep{fenton1990simulation}, wavelets \citep{zeldin1996random}, Hilbert transform techniques \citep{wang2014modeling}, and turning bands methods \citep{mantoglou1982turning}.

The setting outlined in the previous two paragraphs leads, ultimately, to risk characterized as a multidimensional integral over the parametric uncertainty description (input), with uncertainty propagation (output) translating to estimation of the relevant statistics (estimation of integrals representing moments or failure probabilities with respect to different limit states). Through proper definition of the statistics of interest, different attitudes towards risk can be further described, including risk aversion which is especially relevant for NHE applications \citep{cha2012riskaverse}. The aforementioned integral is frequently expressed with respect to the conditional distributions of the different resultant risk components \citep{goulet2007evaluation, barbato2013performancebased}, e.g., {hazard / response given hazard / consequences given response}. This represents merely a simplification for risk quantification purposes as it allows for the decoupling of the different components. Even when this simplification is invoked, risk fundamentally originates from the uncertainty in the model parameters of the problem formulation (including hazard/system/performance description), quantified by assigning a probability distribution to them, representing the UQ input. Note: in many NHE applications, some aspects of the performance (and of the associated risk) are described by utilizing resultant statistical models instead of explicitly addressing the underlying sources of uncertainty.  For example, in PBE, fragility functions are frequently leveraged to describe the combined effect of the uncertainties influencing the parameters of capacity and demand models, while in loss estimation, resultant distributions are leveraged to encompass the multiple sources of uncertainty influencing consequence quantification. In such cases, the integral quantifying risk is expressed with respect to the remaining parametric uncertainty sources.

\section{Uncertainty Propagation}
\label{sec:uq_propagation}

For uncertainty propagation, the traditional approach in NHE has been the use of point estimation methods, either methods that focus on the most probable values of the model parameters like the first-order second moment (FOSM) method \citep{baker2008uncertainty} and its variants \citep{vamvatsikos2013derivation}, or methods that focus on the peaks of the integrand of the probabilistic integral (design points) like the first- and second-order reliability methods (FORM/SORM) \citep{koduru2010feasibility}. Point estimation methods are inherently approximate, with no available means to control their accuracy. Recent advances in computer science and statistics, including the development of innovative MCS techniques, such as Subset Simulation \citep{au2003subset}, Efficient Global Reliability Analysis (EGRA) \citep{bichon2013efficient}, and Adaptive Kriging with Monte Carlo Simulation (AK-MCS) \citep{echard2011akmcs}, have encouraged researchers to rely more heavily on Monte Carlo simulation (MCS) tools for uncertainty propagation in NHE \citep{smith2011monte, taflanidis2011simulationbased, vamvatsikos2014seismic, esposito2015simulation, deb2019parametric}. 

Although point estimation methods do still maintain utility and popularity, NHE trends follow the broader UQ community trends in promoting computer and MCS approaches, as these techniques facilitate high-accuracy uncertainty propagation (unbiased estimation) with limited fundamental constraints on the complexity of the probability and numerical models used. Of course, computational complexity is still a concern for MCS, especially for applications with high-dimensional uncertainties and challenging quantities of interest (such as rare event simulation), creating in many instances practical constraints for the efficient implementation.

The current state-of-the art in NHE for addressing these constraints is to leverage both advanced MCS techniques \citep{au2003subset, li2017system, bansal2018subset} but, more importantly, machine-learning and advanced computational statistics tools \citep{echard2011akmcs, abbiati2017hierarchical, ding2018multifidelity, su2018efficient, wang2018bayesian}. Relevant recent advances for MCS focus on variance reduction techniques, e.g., Latin hypercube sampling \citep{vamvatsikos2014seismic}, stratified sampling \citep{jayaram2010efficient}, importance sampling \citep{papaioannou2018reliability}, Markov chain Monte Carlo methods \citep{au2003subset} and sequential approaches for rare event simulation \citep{jia2017new}, with substantial emphasis on problems with high-dimensional uncertainties \citep{au2003subset, wang2016crossentropybased}. For machine learning, the focus is primarily on use of a variety of surrogate modeling (meta-modeling) techniques \citep{stern2017accelerated, zhang2018adaptive, bernier2019fragility, gentile2020gaussian, le2020neural, zhang2020physicsguided}.

Many machine-learning implementations in NHE fall under the category of direct adoption of techniques developed by the broader UQ community, though a number of studies do address challenges unique to the integration of surrogate modeling in NHE problems, e.g., the need to address high-dimensionality of input when stochastic description is utilized for non-stationary excitations \citep{gidaris2015kriging}.  

Note: the NHE modeling community has been continuously increasing the complexity of the models they adopt. Such high-fidelity numerical models, which are able to capture the behavior of structural, geotechnical and soil--foundation--structural systems all the way to collapse or the brink of collapse, are inherently nonlinear hysteretic (path-dependent) and frequently degrading/softening. Therefore, such models present serious challenges in term of robustness of convergence of the iterative schemes used to integrate their equations of motion.

The significance of these challenges will further increase in the MCS-based UQ context and requires much research effort to overcome. One of the implicit outcomes of this complexity increase is a further increase of the input/output dimensionality in NHE UQ problems. This leads to applications with high-dimensional uncertainties that, traditionally, pose challenges for UQ algorithms \citep{au2003importance, schueller2004critical}. Different approaches have been explored to address this challenge within NHE applications, ranging from specialized MCS algorithms \citep{au2001estimation, wang2016crossentropybased} to the formal integration of dimensional reduction techniques \citep{jia2013kriging}, to the use of global sensitivity analysis indices to the selection of subsets of inputs to emphasize in different MCS schemes \citep{jia2014adaptive}. It is evident, though, that greater research effort will be required on this topic.     

Discussing more broadly the advances in the UQ field, current emphasis is on machine-learning techniques for accelerating UQ computations \citep{murphy2012machine, ghanem2017handbook, tripathy2018deep}. The relevant developments are frequently integrated with advanced MCS techniques, e.g., for topics like rare event simulation \citep{li2011efficient, balesdent2013krigingbased, bourinet2016rareevent}. Regarding machine learning, although some emphasis is being given on the approaches for tuning and validation \citep{mehmani2018concurrent}, the primary focus is on the proper design of the computer simulation experiments (DoE) \citep{kleijnen2008design, picheny2010adaptive, kyprioti2020adaptive} that are used to inform the development of the relevant computational statistics tools.

Adaptive DoE is widely acknowledged to offer substantial advantages in balancing computational efficiency and accuracy for UQ analysis when machine-learning techniques are used, and significant research effort is currently focused on advancing DoE techniques; this remains an open challenge for the community. Note: characteristics of the adaptive DoE depend on the utility of the surrogate model \citep{liu2018survey}, whether it is intended to serve as a global replacement of the original numerical model (i.e., develop the surrogate model first and then leverage it to perform different UQ tasks) or it is used for very specific UQ tasks (e.g., to estimate the reliability index for a specific limit state). 

The concept of model fidelity remains unexplored within the NHE community, but it plays a central role in modern UQ techniques, with a range of algorithms developed to properly integrate hierarchical fidelity models to promote efficient and accurate uncertainty propagation \citep{geraci2017multifidelity, peherstorfer2018survey}. Combination of machine-learning (primarily surrogate modeling) techniques with different fidelity models is also a topic that has been receiving increasing attention for facilitating the use of expensive numerical models in UQ \citep{debaar2015uncertainty, zhou2016active}. In the NHE setting, discussions on explicitly exploiting model fidelity for risk estimation are very limited; therefore, the community still heavily emphasizes use of high-fidelity models while still examining how different levels of simulation fidelity and the use of reduced order models can be properly combined to promote efficient and accurate risk estimation. Undoubtedly, multi-fidelity Monte Carlo and hierarchical surrogate modeling techniques constitute important opportunity areas for advancing UQ analysis in NHE.

Another important aspect of uncertainty propagation is the concept of sensitivity analysis. In NHE, this has been primarily implemented as local sensitivity analysis, i.e., estimation of gradient information \citep{haukaas2007methods, gu2009finite}, since this fits well with the point estimation methods used frequently for calculation of statistics (facilitating the identification of design points). Within the UQ setting, global sensitivity analysis is more relevant \citep{sobol1990sensitivity, saltelli2002making, rahman2016fsensitivity}, as it allows identification of the relative importance of the different sources of uncertainty, offering insights with respect to both accelerating UQ computations (facilitating, in particular, the use of high-dimensional applications as discussed earlier), as well as providing additional understanding of the critical factors impacting the overall risk. Though global sensitivity analysis can be particularly useful for hazard applications \citep{vetter2012global}, it is currently receiving limited practical interest within NHE. Implementations do exist even for all purpose codes; see \citet{bourinet2009review} for example. More formal integration of global sensitivity analysis tools within the NHE community represents another topical area where advancements should and can be made. The computational cost for global sensitivity analysis---say, calculation of first and higher order sensitivity indexes---is much higher than the cost of simple uncertainty propagation; relevant techniques range from use of quasi-Monte Carlo \citep{saltelli2002making} to surrogate modeling \citep{sudret2008global} to sample-based methods relying on approximation of conditional distributions \citep{li2016efficient, hu2019probability}.  

\section{Model Calibration and Bayesian Inference}
\label{sec:uq_calibration}

Model updating/calibration plays an important role in NHE, with data coming from both experiments (at component or system level) experiments and observations (at system level) during actual excitation conditions or post-excitation. Within the UQ setting, the current standard to perform this updating is Bayesian inference \citep{beck2010bayesian, kontoroupi2017online}. Using observation data, Bayesian inference can be leveraged to provide different type of outputs/results \citep{beck2013prior} through the following three tasks: (1) identifying the most probable model parameters or even update the entire probability density function for these parameters (obtain posterior distributions); (2) performing posterior predictive analysis and updating the risk using the new information; and (3) when different numerical models are examined, identifying the probability of each of them (as inferred by the data) to either select the most appropriate or calculate the weights when all of them will be used in a model averaging setting (model class selection).

Typical implementation refers to model parameter updating, which is traditionally viewed as model calibration. Model class selection is less frequently used, especially within NHE community applications. Still, Bayesian model class selection offers a comprehensive tool for evaluating appropriateness of different models \citep{muto2008bayesian}. For NHE, such applications can be integrated with health monitoring tools \citep{oh2018bayesian}. Ideally, model parameter updating should be performed accounting for pertinent sources of real-world uncertainties (e.g., noisy input--output measurements, uncertainty in model parameters, model form uncertainty, and environmental variability). Formulating the likelihood equation to account for such uncertainties is a crucial part of the Bayesian model updating/calibration process. The so-called Kennedy O'Hagan framework has emerged as a robust approach to account for all these uncertainties, especially the model form error \citep{kennedy2001bayesian}. 

From a computational perspective, Bayesian updating may correspond to a significant computational burden, especially when complex finite-element models are utilized. Such finite-element models contain numerous unknown parameters that drastically increase the computational cost of the Bayesian updating process. This necessitates the use of identifiability and sensitivity analyses prior to model updating to select the most significant parameters to use in the updating process. This step substantially improves the calibration process run-time of complex finite-element models with numerous parameters, resulting in better parameter estimation results \citep{ramancha2021bayesianupdating}. In addition, non-identifiable parameters result in non-unique parameter estimates \citep{ramancha2020nonunique}. Fisher information-based local identifiability analysis and variance-based global sensitivity analysis are commonly used methods for parameter screening to identify the most significant and identifiable parameters \citep{ramancha2021bayesianupdating}.

Beyond this critical dimensionality reduction, a variety of algorithmic approaches are commonly used to address the computational complexity in Bayesian updating applications. Common approaches include the use of advanced MCS techniques to reduce the total number of simulations needed \citep{quiroz2018speeding}, the integration of meta-modeling to approximate the complex system model \citep{angelikopoulos2015xtmcmc, giovanis2017bayesian, wang2019reliabilitybased, zhang2019accelerating}, or the use of direct differentiation tools to accelerate computations \citep{astroza2017batch}. The second candidate implementation, the use of surrogate modeling techniques, has undoubtedly significant potential in accommodating the use of complex models within Bayesian calibration schemes.     

Bayesian updating may rely on point estimates, which is equivalent to identifying and using only the most probable (based on the observation data) model parameter values (expressed as a nonlinear optimization problem), or leveraging the entire posterior distribution (expressed as a problem of sampling from this distribution). For the latter, Markov Chain Monte Carlo (MCMC) techniques need to be used for any of the three tasks entailed in Bayesian inference \citep{catanach2018bayesian}. In particular, transitional MCMC (TMCMC) is a versatile method for sampling the posterior distribution \citep{ching2007transitional, betz2016transitional}. Due to its computationally parallel nature, it is ideal for Bayesian inference of computationally expensive FE models \citep{ramancha2021bayesian, ramancha2021bayesianupdating}. For problems involving inference for dynamic models (pertinent to the majority of applications in NHE), updating can be done in batch mode, using all observation data at once, or recursive mode, sequentially updating model characteristics during the time history for the observations \citep{astroza2017batch, ramancha2021bayesianupdating}.

The batch approach is a direct implementation of the broader Bayesian inference framework. The recursive implementation typically leads to filtering approaches, including Kalman filters (KF) and its variants (Extended KF or Unscented KF), that rely on linear or Gaussian assumptions \citep{astroza2017batch, kontoroupi2017online, erazo2018bayesian}, and particle filters (PF) that reproduce the work of KF in nonlinear and/or non-Gaussian environment by a sequential MCS approach \citep{chatzi2009unscented, wei2013dynamic, olivier2017particle}. This recursive approach is used primarily for real-time or online applications, focusing mainly on the most probable parameter values. 

\section{Design under Uncertainty}
\label{sec:uq_design}

In NHE, design under uncertainty has been traditionally expressed as a reliability-based design optimization (RBDO) \citep{spence2012large, chun2019systemreliabilitybased} or as a robust design optimization (RDO) \citep{greco2015robust} problem. Some recent approaches deviate from this pattern and follow directly PBE advances, formulating the design problem with respect to life-cycle cost and performance objectives \citep{shin2014minimum}, and even adopting multiple probabilistic criteria to represent different risk-attitudes \citep{haukaas2012reliabilitybased, gidaris2017multiobjective, li2018probabilistic, deb2019simplified}. Practical applications focus on design of supplemental dissipative devices \citep{shin2014minimum, gidaris2017multiobjective, altieri2018reliabilitybased} member-sizing \citep{huang2015performancebased, suksuwan2018optimization}, or even  topology-based optimization of structural systems \citep{bobby2017reliabilitybased, zhu2017topology}.

With respect to the solution of the corresponding optimization problem, the NHE community follows broader UQ trends. Design under uncertainty optimization problems undoubtedly present significant computational challenges since they combine two tasks, each with considerable computational burden: uncertainty propagation and optimization. Discussed next is how uncertainty propagation is handled within this coupled problem.

Common approaches, especially within context of RBDO and RDO, typically rely on approximate point-estimation methods like FORM/SORM \citep{papadimitriou2018reliability} with some sort of decoupling of the optimization/uncertainty-propagation loops to accelerate convergence \citep{beyer2007robust}. Over the past decade, advances in the use of simulation techniques within UQ have created new opportunities to incorporate MCS techniques in solving design under uncertainty problems \citep{spall2003introduction, flint2016developing}, lifting some of the traditionally associated computational barriers. Greater emphasis is continuously placed on solving design under uncertainty problems using advanced Monte Carlo techniques \citep{medina2014adaptive}, which are frequently coupled with an intelligent integration of surrogate modeling tools \citep{dubourg2011reliabilitybased, bichon2013efficient, zhang2018adaptive}. This trend is expected to continue since computer science and machine-learning advances have dramatically altered the computational complexity for leveraging MCS for design optimization under uncertainty, offering an attractive alternative to traditional approaches that relied on the approximate (but highly efficient) point estimation methods.   

\section{Relevant Software}
\label{sec:uq_tools}

Beyond specific UQ algorithms developed by individual researchers and shared in repositories like GitHub or MATLAB's File Exchange, two other important UQ software categories exist:

\begin{itemize}
    \item Libraries integrated with existing modeling tools appropriate for NHE analysis, like the general purpose finite-element model reliability tools offered through FERUM \citep{bourinet2009review}. These libraries frequently address a specific type of UQ analysis, e.g., direct MCS or reliability estimation.

    \item Software that approache UQ analysis with a broad brush that could be appropriate for use in NHE applications (but as of yet has not necessarily been developed specifically for that purpose). Such software typically covers the entire range of UQ analysis, with continuous integration of the relevant state-of-the art advances. They are composed of scientific modules that perform different UQ tasks, connected through the main software engine and, in addition, are commonly equipped with an appropriate GUI. 
\end{itemize}
	
The last category of UQ software packages is of greater interest, especially since it covers the entire domain of a rapidly expanding field and facilitates the integration of the relevant developments, which typically leverage different classes of tools, e.g., rare-event simulation using surrogate models with adaptive refinement. UQ software programs typically address the following tasks:

\begin{itemize}

    \item \textbf{Probabilistic modeling} This pertains to standard uncertainty characterization, extending from simple parametric description to stochastic characterization (including dimensionality reduction), and represents the input to the UQ software. 
    
    These tasks encompass direct MCS with Latin hypercube sampling, use of point estimation methods (FORM/SORM),  variance reduction, and rare event simulation. UQ outputs considered correspond typically to statistical moments, probabilities of exceedance for different limit states or fitted distributions. The numerous software programs adopt different tools for the aforementioned tasks and most lack a complete adaptive implementation; some degree of competency on behalf of the end-user for selecting appropriate algorithms and parameters is assumed. Recently, many types of software have begun to integrate multi-fidelity MCS approaches;

    \item \textbf{Surrogate modeling} Common classes of meta-models used include Gaussian processes, polynomial chaos, support vector machines, and radial basis functions. The developed surrogate models can be then leveraged within the software to accelerate computations for other UQ tasks. Adaptive DoE options are typically available and used frequently; standard DoE approaches are not, however, necessarily tailored to the specific UQ task of interest to the end-user. Most software programs sacrifice robustness (an approach that is reliable and is independent of the end-user competency) for efficiency (the ability to develop high-accuracy meta-models with the least number of simulation experiments);

    \item \textbf{Global sensitivity analysis} This is typically performed through calculation of Sobol indices (UQ output) using some approximate (quasi-Monte Carlo) technique or surrogate modeling (polynomial chaos expansion);

    \item \textbf{Data analysis and model calibration} with some emphasis on Bayesian inference techniques. Although Bayesian updating is very common, model class selection is not. Addressing modeling complexity remains a bigger challenge for Bayesian inference applications since integrating meta-modeling techniques is not trivial. The challenge here is to establish a fully automated integration that can address different degrees of competency for the end-user and a wide range of application problems with certain degree of robustness (impact of metamodel error). For problems with dynamical models, the typical approach is to use the batch updating method since that leads to a common broader Bayesian inference framework; and

    \item \textbf{Design-under-uncertainty} Though this is not a common option, some software programs do offer the ability to perform some form of optimization under uncertainty; most of these programs are applicable to RBDO and RDO problems. Integrating state-of-the-art MCS techniques in this setting remains a challenge. Implementations are typically computationally expensive or rely on approximate approaches for the uncertainty propagation.
\end{itemize}

\noindent Out of the many UQ software programs that currently exist, the programs listed below are worth direct mention as they represent the state-of-the-art: 

\paragraph{DAKOTA} Developed by the Sandia National Laboratory and written in C++, \citeprgm{DAKOTA} is widely considered as the standard for UQ software and delivers both state-of-the-art research and robust, usable tools for optimization and UQ \citep{adams2009dakota}. It has a range of algorithms for all aforementioned UQ tasks and has a wide community that supports its continuous development. 

\paragraph{OpenTURNS} It is an open-source (C++/Python library) initiative for the treatment of uncertainties and risk in simulations \citep{andrianov2007open}. It addresses all aforementioned UQ tasks apart from design under uncertainty. 

\paragraph{UQLab} Developed at ETH Zürich, \citeprgm{UQLab} is a MATLAB-based general purpose UQ framework \citep{marelli2014uqlab}. Like OpenTURNS it addresses all the aforementioned UQ tasks apart from design under uncertainty. 

\paragraph{OpenCOSSAN} 
\citeprgm{OpenCOSSAN} is the MATLAB based open-source version of the commercial software \citeprgm{COSSAN-X} \citep{patelli2017cossan}, which was initially developed to integrate UQ and reliability techniques within FEM analysis, with modules that extend across all aforementioned UQ tasks.

\paragraph{UQpy} It is an continuously expanding open source (Python-based) library of tools for UQ analysis \citep{olivier2020uqpy}. It addresses all UQ tasks apart from design-under-uncertainty (at least in its current form).

\paragraph{MUQ} 
\citeprgm{MUQ} is an open source (C++/Python library) collection of tools for UQ analysis, with emphasis on probabilistic modeling, Bayesian inference, and surrogate modeling. Although developments in MUQ have been slightly more accelerated recently, overall it has not kept pace with the other open-source initiatives mentioned above.


Beyond these specific software and open-source tool collections, there is an increasing number of Python-based open-source libraries that are offered by researchers for UQ analysis, including UQ-Pyl (http://www.uq-pyl.com/), FilterPy (https://github.com/rlabbe/filterpy), and Surrogate Modeling Toolbox (https://github.com/SMTorg/SMT). Most of them are focused on specialized UQ tasks (or groups of tasks) and do not intend to establish a generalized UQ analysis workflow. The degree of ongoing improvement and bug fixes also varies substantially between these efforts. 